{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ANRP_ir.ipynb",
      "authorship_tag": "ABX9TyNk9alkw4uzZwQyV/cjRzVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RezaRasti79/Hosh-pr/blob/main/ANRP_ir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "BFQnfHRB0Err"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/augmentedstartups/yolov7.git\n",
        "%cd yolov7\n",
        "!pip install -r requirements.txt\n",
        "!pip install roboflow"
      ],
      "metadata": {
        "id": "eQHwS1Ul0Hqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"4F45mvdcFntkwR9lvdbk\")\n",
        "project = rf.workspace(\"akh78\").project(\"anpr_ir-ax9dp\")\n",
        "dataset = project.version(1).download(\"yolov7\")"
      ],
      "metadata": {
        "id": "0RwDdCkN0L3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ],
      "metadata": {
        "id": "Ke7wgGvZ0L0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 15 --data /content/gdrive/MyDrive/yolov7/ANPR_ir-1/data.yaml --weights 'yolov7.pt' --de"
      ],
      "metadata": {
        "id": "gY-MCQHl0Lx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "CniH86xI0Lu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/F1_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/PR_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/confusion_matrix.png\", width=500, height=500))"
      ],
      "metadata": {
        "id": "_w19mZNV0LsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/gdrive/MyDrive/yolov7/runs/train/exp/weights/best.pt --conf 0.1 --source /content/gdrive/MyDrive/yolov7/ANPR_ir-1/test/"
      ],
      "metadata": {
        "id": "Tgko6mpW0Lpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIjpru9V0Lm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lysVFbi0Lj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfleFYq40Lg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_sort_realtime\n",
        "!pip install \"paddleocr>=2.0.1\""
      ],
      "metadata": {
        "id": "7j9crhPV0Ld-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fileinput\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "import torch\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort"
      ],
      "metadata": {
        "id": "gtGtvNja0LbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('weights.pt'):\n",
        "    weights_url = 'https://archive.org/download/anpr_weights/weights.pt'\n",
        "    os.system(f'wget {weights_url}')\n",
        "\n",
        "if not os.path.isdir('examples'):\n",
        "    examples_url = 'https://archive.org/download/anpr_examples_202208/examples.tar.gz'\n",
        "    os.system(f'wget {examples_url}')\n",
        "    os.system('tar -xvf examples.tar.gz')\n",
        "    os.system('rm -rf examples.tar.gz')\n",
        "\n",
        "def prepend_text(filename: Union[str, Path], text: str):\n",
        "    with fileinput.input(filename, inplace=True) as file:\n",
        "        for line in file:\n",
        "            if file.isfirstline():\n",
        "                print(text)\n",
        "            print(line, end=\"\")\n",
        "\n",
        "if not os.path.isdir('yolov7'):\n",
        "    yolov7_repo_url = 'https://github.com/WongKinYiu/yolov7'\n",
        "    os.system(f'git clone {yolov7_repo_url}')\n",
        "    # Fix import errors\n",
        "    for file in ['yolov7/models/common.py', 'yolov7/models/experimental.py', 'yolov7/models/yolo.py', 'yolov7/utils/datasets.py']:\n",
        "         prepend_text(file, \"import sys\\nsys.path.insert(0, './yolov7')\")"
      ],
      "metadata": {
        "id": "hMVepRyb0LYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from yolov7.models.experimental import attempt_load\n",
        "from yolov7.utils.general import check_img_size\n",
        "from yolov7.utils.torch_utils import select_device, TracedModel\n",
        "from yolov7.utils.datasets import letterbox\n",
        "from yolov7.utils.general import non_max_suppression, scale_coords\n",
        "from yolov7.utils.plots import plot_one_box"
      ],
      "metadata": {
        "id": "__CJn5QT0LVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = 'weights.pt'\n",
        "device_id = 'cpu'\n",
        "image_size = 640\n",
        "trace = True\n",
        "\n",
        "# Initialize\n",
        "device = select_device(device_id)\n",
        "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "# Load model\n",
        "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "stride = int(model.stride.max())  # model stride\n",
        "imgsz = check_img_size(image_size, s=stride)  # check img_size\n",
        "\n",
        "if trace:\n",
        "    model = TracedModel(model, device, image_size)\n",
        "\n",
        "if half:\n",
        "    model.half()  # to FP16\n",
        "    \n",
        "if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "wl2D14Dm0LSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_image_path = \"/content/gdrive/MyDrive/DL_projects_colab/ANPR/2.jpg\"    \n",
        "source_image = cv.imread(source_image_path)\n",
        "print(source_image.shape)\n",
        "# Padded resize\n",
        "img_size = 640\n",
        "stride = 32\n",
        "img = letterbox(source_image, img_size, stride=stride)[0]"
      ],
      "metadata": {
        "id": "Gu2XlxTv0LOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert\n",
        "img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "img = np.ascontiguousarray(img)\n",
        "img = torch.from_numpy(img).to(device)\n",
        "img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    # Inference\n",
        "    pred = model(img, augment=True)[0]\n",
        "\n",
        "# Apply NMS\n",
        "pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n"
      ],
      "metadata": {
        "id": "vBcJve-U0LLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plate_detections = []\n",
        "det_confidences = []\n",
        "\n",
        "# Process detections\n",
        "for i, det in enumerate(pred):  # detections per image\n",
        "    if len(det):\n",
        "        # Rescale boxes from img_size to im0 size\n",
        "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n",
        "\n",
        "        # Return results\n",
        "        for *xyxy, conf, cls in reversed(det):\n",
        "            coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n",
        "            plate_detections.append(coords)\n",
        "            det_confidences.append(conf.item())"
      ],
      "metadata": {
        "id": "C14MH2pL0LIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plate_detections"
      ],
      "metadata": {
        "id": "se4RK-m_0LFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for coords in plate_detections:\n",
        "  cv.rectangle(source_image, (coords[0], coords[1]), (coords[2], coords[3]), (0,255,0), 2)"
      ],
      "metadata": {
        "id": "Vw9ixgsV0LC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(source_image)"
      ],
      "metadata": {
        "id": "JfR--hzX0K_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.imwrite(\"/content/examples/det_image_1.jpg\", source_image)"
      ],
      "metadata": {
        "id": "n4VC73Zn0K9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_plate(source_image):\n",
        "    # Padded resize\n",
        "    img_size = 640\n",
        "    stride = 32\n",
        "    img = letterbox(source_image, img_size, stride=stride)[0]\n",
        "\n",
        "    # Convert\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        # Inference\n",
        "        pred = model(img, augment=True)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n",
        "\n",
        "    plate_detections = []\n",
        "    det_confidences = []\n",
        "    \n",
        "    # Process detections\n",
        "    for i, det in enumerate(pred):  # detections per image\n",
        "        if len(det):\n",
        "            # Rescale boxes from img_size to im0 size\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n",
        "\n",
        "            # Return results\n",
        "            for *xyxy, conf, cls in reversed(det):\n",
        "                coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n",
        "                plate_detections.append(coords)\n",
        "                det_confidences.append(conf.item())\n",
        "\n",
        "    return plate_detections, det_confidences\n",
        "\n",
        "def get_plates_from_image(input):\n",
        "    if input is None:\n",
        "        return None\n",
        "    plate_detections, det_confidences = detect_plate(input)\n",
        "    plate_texts = []\n",
        "    ocr_confidences = []\n",
        "    detected_image = deepcopy(input)\n",
        "    for coords in plate_detections:\n",
        "        plate_region = crop(input, coords)\n",
        "        plate_text, ocr_confidence = ocr_plate(plate_region)\n",
        "        plate_texts.append(plate_text)\n",
        "        ocr_confidences.append(ocr_confidence)\n",
        "        plot_one_box(coords, detected_image, label=plate_text, color=[0, 150, 255], line_thickness=2)\n",
        "    return detected_image\n",
        "    \n",
        "def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=2.0, threshold=0):\n",
        "    blurred = cv2.GaussianBlur(image, kernel_size, sigma)\n",
        "    sharpened = float(amount + 1) * image - float(amount) * blurred\n",
        "    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
        "    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
        "    sharpened = sharpened.round().astype(np.uint8)\n",
        "    if threshold > 0:\n",
        "        low_contrast_mask = np.absolute(image - blurred) < threshold\n",
        "        np.copyto(sharpened, image, where=low_contrast_mask)\n",
        "    return sharpened\n",
        "\n",
        "def crop(image, coord):\n",
        "    cropped_image = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
        "    return cropped_image\n"
      ],
      "metadata": {
        "id": "mrP3pXz40K6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Klukhv-H0K3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNI55xs40K0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0kR-3Rsi0KxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgrehs0H0KuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nk7K13u50KrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YbWITJa0KoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SB7zGvJy0Kk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIY6GJQv0KiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5fcmosj0KfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYyMtlm70KcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtj2L3k40KZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-bWwYM80KWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05ROuoKe0KTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1uc2RTFm0KQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mB0ykSqe0KNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdB4JiLo0KK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTQIrlPi0KIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfezMJR10KFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ws58MTF70KCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kB7b0P_70J_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPMFAEBJ0J7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_uTHZmQ0Jua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}